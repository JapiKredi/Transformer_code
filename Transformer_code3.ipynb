{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbe2dKT90QMaIc8cQG/hWi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JapiKredi/Transformer_code/blob/main/Transformer_code3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx-zjb9NE9tc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the positional encoding module\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n"
      ],
      "metadata": {
        "id": "vF51FzXNFaXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformer encoder layer\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask)\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n"
      ],
      "metadata": {
        "id": "4XUHi4M0FfVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformer encoder\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, n_heads, d_ff, dropout):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        return src\n",
        "\n"
      ],
      "metadata": {
        "id": "hfYDfwNaFkm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformer decoder layer\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.linear3 = nn.Linear(d_model, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
        "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2, _ = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask)\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(F.relu(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n"
      ],
      "metadata": {
        "id": "G1HltqOAFoap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformer decoder\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, n_heads, d_ff, dropout):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([TransformerDecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, memory, tgt_mask, memory_mask)\n",
        "        return tgt\n",
        "\n"
      ],
      "metadata": {
        "id": "FiQXM4kkFr8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformer\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, src_pad_idx, tgt_pad_idx, d_model=512, num_layers=6, n_heads=8, d_ff=2048, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = TransformerEncoder(num_layers, d_model, n_heads, d_ff, dropout)\n",
        "        self.decoder = TransformerDecoder(num_layers, d_model, n_heads, d_ff, dropout)\n",
        "        self.src_embed = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embed = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.generator = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.tgt_pad_idx = tgt_pad_idx\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        tgt_mask = (tgt != self.tgt_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_length = tgt.size(1)\n",
        "        tgt_sub_mask = torch.tril(torch.ones((tgt_length, tgt_length), device=tgt.device)).bool()\n",
        "        tgt_mask = tgt_mask & tgt_sub_mask\n",
        "        return tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        tgt_mask = self.make_tgt_mask(tgt)\n",
        "\n",
        "        src = self.src_embed(src)\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "\n",
        "        memory = self.encoder(src, src_mask)\n",
        "        output = self.decoder(tgt, memory, tgt_mask, src_mask)\n",
        "\n",
        "        output = self.generator(output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "bIhshIp3FuWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "src_vocab_size = 10000\n",
        "tgt_vocab_size = 10000\n",
        "src_pad_idx = 0\n",
        "tgt_pad_idx = 0\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, src_pad_idx, tgt_pad_idx)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F63oB6cFwfC",
        "outputId": "f80f7d5a-a9eb-499c-ac6b-30804b406a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (multihead_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (linear3): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (src_embed): Embedding(10000, 512)\n",
            "  (tgt_embed): Embedding(10000, 512)\n",
            "  (generator): Linear(in_features=512, out_features=10000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Prepare Data\n",
        "# TODO: Prepare your dataset and create data loaders\n",
        "\n",
        "# Step 2: Instantiate the Model\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, src_pad_idx, tgt_pad_idx)\n",
        "\n",
        "# Step 3: Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Step 4: Training Loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for src, tgt in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt[:, :-1])  # Ignore the last token of target sequence for training\n",
        "        loss = criterion(output.transpose(1, 2), tgt[:, 1:])  # Compute loss, ignoring the <sos> token\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Step 5: Evaluation\n",
        "model.eval()\n",
        "total_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for src, tgt in val_loader:\n",
        "        output = model(src, tgt[:, :-1])  # Ignore the last token of target sequence for evaluation\n",
        "        loss = criterion(output.transpose(1, 2), tgt[:, 1:])  # Compute loss, ignoring the <sos> token\n",
        "        total_loss += loss.item()\n",
        "val_loss = total_loss / len(val_loader)\n",
        "\n",
        "# Step 6: Inference\n",
        "# TODO: Use the trained model for inference on new data\n"
      ],
      "metadata": {
        "id": "ckxa2KEGFxJ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}